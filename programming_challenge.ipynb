{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rKwQRUBgRP0c"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the JSON records from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Save the deduplicated JSON records to a file\n",
        "def save_json(data, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "# Deduplicate the records based on the given rules\n",
        "def deduplicate(records):\n",
        "    unique_by_id = {}\n",
        "    unique_by_email = {}\n",
        "    log = []\n",
        "\n",
        "    for record in records:\n",
        "        record_id = record.get(\"_id\")\n",
        "        record_email = record.get(\"email\")\n",
        "        record_date = datetime.fromisoformat(record.get(\"entryDate\"))\n",
        "\n",
        "        # Determine the unique key for deduplication\n",
        "        conflict_key = record_id if record_id in unique_by_id else record_email\n",
        "\n",
        "        if conflict_key in unique_by_id or conflict_key in unique_by_email:\n",
        "            existing_record = unique_by_id.get(conflict_key) or unique_by_email.get(conflict_key)\n",
        "            existing_date = datetime.fromisoformat(existing_record[\"entryDate\"])\n",
        "\n",
        "            if record_date > existing_date or (record_date == existing_date and records.index(record) > records.index(existing_record)):\n",
        "                # Log the changes\n",
        "                changes = []\n",
        "                for key in record.keys():\n",
        "                    if record[key] != existing_record.get(key):\n",
        "                        changes.append({\"field\": key, \"from\": existing_record.get(key), \"to\": record[key]})\n",
        "\n",
        "                log.append({\n",
        "                    \"source_record\": existing_record,\n",
        "                    \"output_record\": record,\n",
        "                    \"field_changes\": changes,\n",
        "                })\n",
        "\n",
        "                # Update the record\n",
        "                if record_id in unique_by_id:\n",
        "                    unique_by_id[record_id] = record\n",
        "                if record_email in unique_by_email:\n",
        "                    unique_by_email[record_email] = record\n",
        "        else:\n",
        "            # Add the record as new\n",
        "            if record_id:\n",
        "                unique_by_id[record_id] = record\n",
        "            if record_email:\n",
        "                unique_by_email[record_email] = record\n",
        "\n",
        "    # Return the deduplicated records and the log\n",
        "    deduplicated_records = list({**unique_by_id, **unique_by_email}.values())\n",
        "    return deduplicated_records, log\n",
        "\n",
        "# Main function to process the file\n",
        "def main():\n",
        "    input_file = \"/content/leads.json\"\n",
        "    output_file = \"deduplicated_leads.json\"\n",
        "    log_file = \"deduplication_log.json\"\n",
        "\n",
        "    # Load the input records\n",
        "    data = load_json(input_file)\n",
        "    records = data.get(\"leads\", [])\n",
        "\n",
        "    # Deduplicate the records\n",
        "    deduplicated_records, log = deduplicate(records)\n",
        "\n",
        "    # Save the deduplicated records and the log\n",
        "    save_json({\"leads\": deduplicated_records}, output_file)\n",
        "    save_json(log, log_file)\n",
        "\n",
        "main()\n"
      ]
    }
  ]
}